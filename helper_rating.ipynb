{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "TOKEN_MODE = 'Plain'\n",
    "NGRAM = 1\n",
    "MAX_VOCAB_SIZE = 300000  # Set maximum vocabulary size\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 3\n",
    "EMBED_DIM = 100\n",
    "OPTIMIZER = 'ADAM'\n",
    "LAERNING_ANNEALING = 'YES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize path to the directory where the files are stored\n",
    "path_train_pos = '/train/pos'\n",
    "path_train_neg = '/train/neg'\n",
    "path_test_pos = '/test/pos'\n",
    "path_test_neg = '/test/neg'\n",
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the loaded files are train_pos 12500 train_neg 12500 test_pos 12500 test_neg 12500\n",
      "The length of the loaded labels are train_label_pos 12500 train_label_neg 12500 test_label_pos 12500 test_label_neg 12500\n"
     ]
    }
   ],
   "source": [
    "# Load files\n",
    "train_pos = []\n",
    "train_neg = []\n",
    "test_pos = []\n",
    "test_neg = []\n",
    "train_label_pos = []\n",
    "train_label_neg = []\n",
    "test_label_pos = []\n",
    "test_label_neg = []\n",
    "os.chdir(current_path+path_train_pos)\n",
    "for file in os.listdir():\n",
    "    rate = file.strip('.txt').split('_')[-1]\n",
    "    fin = open(file, 'r')\n",
    "    train_pos.append(fin.read())\n",
    "    train_label_pos.append(int(rate))\n",
    "    fin.close()\n",
    "\n",
    "os.chdir(current_path+path_train_neg)\n",
    "for file in os.listdir():\n",
    "    rate = file.strip('.txt').split('_')[-1]\n",
    "    fin = open(file, 'r')\n",
    "    train_neg.append(fin.read())\n",
    "    train_label_neg.append(int(rate))\n",
    "    fin.close()\n",
    "\n",
    "os.chdir(current_path+path_test_pos)\n",
    "for file in os.listdir():\n",
    "    rate = file.strip('.txt').split('_')[-1]\n",
    "    fin = open(file, 'r')\n",
    "    test_pos.append(fin.read())\n",
    "    test_label_pos.append(int(rate))\n",
    "    fin.close()\n",
    "\n",
    "os.chdir(current_path+path_test_neg)\n",
    "for file in os.listdir():\n",
    "    rate = file.strip('.txt').split('_')[-1]\n",
    "    fin = open(file, 'r')\n",
    "    test_neg.append(fin.read())\n",
    "    test_label_neg.append(int(rate))\n",
    "    fin.close()\n",
    "    \n",
    "# Check of the test has been correctly loaded\n",
    "print('The length of the loaded files are train_pos {} train_neg {} test_pos {} test_neg {}'.format(len(train_pos),len(train_neg),len(test_pos),len(test_neg)))\n",
    "print('The length of the loaded labels are train_label_pos {} train_label_neg {} test_label_pos {} test_label_neg {}'.format(len(train_label_pos),len(train_label_neg),len(test_label_pos),len(test_label_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the loaded files are train_pos 10000 train_neg 10000 val_pos 2500 val_neg 2500\n",
      "The length of the loaded files are train_label_pos 10000 train_label_neg 10000 val_label_pos 2500 val_label_neg 2500\n"
     ]
    }
   ],
   "source": [
    "# Split the training set to training set and validation set\n",
    "TRAIN_VAL_SPLIT = 10000\n",
    "val_pos = []\n",
    "val_neg = []\n",
    "val_label_pos = []\n",
    "val_label_neg = []\n",
    "val_pos = train_pos[TRAIN_VAL_SPLIT:]\n",
    "val_label_pos = train_label_pos[TRAIN_VAL_SPLIT:]\n",
    "train_pos = train_pos[:TRAIN_VAL_SPLIT]\n",
    "train_label_pos = train_label_pos[:TRAIN_VAL_SPLIT]\n",
    "\n",
    "val_neg = train_neg[TRAIN_VAL_SPLIT:]\n",
    "val_label_neg = train_label_neg[TRAIN_VAL_SPLIT:]\n",
    "train_neg = train_neg[:TRAIN_VAL_SPLIT]\n",
    "train_label_neg = train_label_neg[:TRAIN_VAL_SPLIT]\n",
    "\n",
    "# Check the correctness of the split\n",
    "print('The length of the loaded files are train_pos {} train_neg {} val_pos {} val_neg {}'.format(len(train_pos),len(train_neg),len(val_pos),len(val_neg)))\n",
    "print('The length of the loaded files are train_label_pos {} train_label_neg {} val_label_pos {} val_label_neg {}'.format(len(train_label_pos),len(train_label_neg),len(val_label_pos),len(val_label_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine positive and negative datasets\n",
    "train_set = train_pos + train_neg\n",
    "val_set = val_pos + val_neg\n",
    "test_set = test_pos + test_neg\n",
    "\n",
    "train_label = train_label_pos + train_label_neg\n",
    "val_label = val_label_pos + val_label_neg\n",
    "test_label = test_label_pos + test_label_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations, stopwords and perform stemming\n",
    "punctuations = string.punctuation\n",
    "stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def tokenize_sentence_advanced(inputStr, N):\n",
    "    token_results = []\n",
    "    tokens = word_tokenize(inputStr)\n",
    "    tokens = [ps.stem(token.lower()) for token in tokens if (token not in punctuations and token not in stopwords)]\n",
    "    if N == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        counter = 0\n",
    "        while counter < len(tokens)-N+1:\n",
    "            temp = ''\n",
    "            for i in range(N):\n",
    "                temp += str(tokens[counter+i]) + ' '\n",
    "            token_results.append(temp.strip(' '))\n",
    "            counter += 1\n",
    "        return token_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all datasets for ngram\n",
    "def tokenize_dataset_ngram(input_set, N):\n",
    "    all_tokens = []\n",
    "    set_tokens = []\n",
    "    inner_counter = 1\n",
    "    for review in input_set:\n",
    "        token_temp = tokenize_sentence_advanced(review, N)\n",
    "        all_tokens += token_temp\n",
    "        set_tokens.append(token_temp)\n",
    "        if inner_counter % 500 == 0:\n",
    "            print('Finished tokenizing review {}'.format(inner_counter))\n",
    "        inner_counter += 1\n",
    "    return set_tokens, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 20500\n",
      "Finished tokenizing review 21000\n",
      "Finished tokenizing review 21500\n",
      "Finished tokenizing review 22000\n",
      "Finished tokenizing review 22500\n",
      "Finished tokenizing review 23000\n",
      "Finished tokenizing review 23500\n",
      "Finished tokenizing review 24000\n",
      "Finished tokenizing review 24500\n",
      "Finished tokenizing review 25000\n",
      "\n",
      "Finished 1 gram\n",
      "\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 20500\n",
      "Finished tokenizing review 21000\n",
      "Finished tokenizing review 21500\n",
      "Finished tokenizing review 22000\n",
      "Finished tokenizing review 22500\n",
      "Finished tokenizing review 23000\n",
      "Finished tokenizing review 23500\n",
      "Finished tokenizing review 24000\n",
      "Finished tokenizing review 24500\n",
      "Finished tokenizing review 25000\n",
      "\n",
      "Finished 2 gram\n",
      "\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 20500\n",
      "Finished tokenizing review 21000\n",
      "Finished tokenizing review 21500\n",
      "Finished tokenizing review 22000\n",
      "Finished tokenizing review 22500\n",
      "Finished tokenizing review 23000\n",
      "Finished tokenizing review 23500\n",
      "Finished tokenizing review 24000\n",
      "Finished tokenizing review 24500\n",
      "Finished tokenizing review 25000\n",
      "\n",
      "Finished 3 gram\n",
      "\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 20500\n",
      "Finished tokenizing review 21000\n",
      "Finished tokenizing review 21500\n",
      "Finished tokenizing review 22000\n",
      "Finished tokenizing review 22500\n",
      "Finished tokenizing review 23000\n",
      "Finished tokenizing review 23500\n",
      "Finished tokenizing review 24000\n",
      "Finished tokenizing review 24500\n",
      "Finished tokenizing review 25000\n",
      "\n",
      "Finished 4 gram\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Ngram advanced tokenization\n",
    "for N in range(4):\n",
    "    train_tokens, all_tokens = tokenize_dataset_ngram(train_set, N+1)\n",
    "    val_tokens, _ = tokenize_dataset_ngram(val_set, N+1)\n",
    "    test_tokens, _ = tokenize_dataset_ngram(test_set, N+1)\n",
    "    os.chdir(current_path)\n",
    "    pkl.dump(train_tokens, open(\"train_tokens_\" + str(N+1) + '_full' + \".p\", \"wb\"))\n",
    "    pkl.dump(val_tokens, open(\"val_tokens_\" + str(N+1) + '_full' + \".p\", \"wb\"))\n",
    "    pkl.dump(test_tokens, open(\"test_tokens_\" + str(N+1) + '_full' + \".p\", \"wb\"))\n",
    "    pkl.dump(all_tokens, open(\"all_tokens_\" + str(N+1) + '_full' + \".p\", \"wb\"))\n",
    "    print('\\nFinished ' + str(N+1) + \" gram\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stored N-Gram pickle and load into program\n",
    "def extract_n_gram_tokens_full(N):\n",
    "    os.chdir(current_path)\n",
    "    train_tokens = pkl.load(open(current_path + \"/train_tokens_\" + str(N) + '_full' + \".p\", \"rb\"))\n",
    "    val_tokens = pkl.load(open(current_path + \"/val_tokens_\" + str(N) + '_full' + \".p\", \"rb\"))\n",
    "    test_tokens = pkl.load(open(current_path + \"/test_tokens_\" + str(N) + '_full' + \".p\", \"rb\"))\n",
    "    all_tokens = pkl.load(open(current_path + \"/all_tokens_\" + str(N) + '_full' + \".p\", \"rb\"))\n",
    "    return train_tokens, val_tokens, test_tokens, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 1gram \n",
    "train_tokens, val_tokens, test_tokens, all_tokens = extract_n_gram_tokens_full(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 2gram\n",
    "train_tokens, val_tokens, test_tokens, all_tokens = extract_n_gram_tokens_full(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 3gram\n",
    "train_tokens, val_tokens, test_tokens, all_tokens = extract_n_gram_tokens_full(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 4gram\n",
    "train_tokens, val_tokens, test_tokens, all_tokens = extract_n_gram_tokens_full(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('br', 81029),\n",
       " ('i', 65136),\n",
       " (\"'s\", 49454),\n",
       " ('movi', 39569),\n",
       " ('film', 37063),\n",
       " ('the', 36204),\n",
       " (\"''\", 26615),\n",
       " (\"n't\", 26558),\n",
       " ('``', 26402),\n",
       " ('one', 21423),\n",
       " ('like', 17638),\n",
       " ('it', 15315),\n",
       " ('time', 12214),\n",
       " ('thi', 11994),\n",
       " ('good', 11821),\n",
       " ('make', 11541),\n",
       " ('get', 11162),\n",
       " ('see', 11130),\n",
       " ('charact', 11012),\n",
       " ('watch', 10888),\n",
       " ('would', 10761),\n",
       " ('stori', 10159),\n",
       " ('even', 10103),\n",
       " ('...', 9679),\n",
       " ('realli', 9332),\n",
       " ('scene', 8099),\n",
       " ('show', 8014),\n",
       " ('well', 7902),\n",
       " ('look', 7783),\n",
       " ('much', 7752)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(all_tokens).most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve special index for padding and unknown words\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "# The method to build vocabulary\n",
    "def build_vocab(all_tokens):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    tokens, frequency = zip(*token_counter.most_common(MAX_VOCAB_SIZE))\n",
    "    id2token = list(tokens)\n",
    "    token2id = dict(zip(tokens, range(2,2+len(tokens))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return id2token, token2id\n",
    "\n",
    "id2token, token2id = build_vocab(all_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 6300 ; token gloss\n",
      "Token gloss; token id 6300\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished review NO 200\n",
      "finished review NO 400\n",
      "finished review NO 600\n",
      "finished review NO 800\n",
      "finished review NO 1000\n",
      "finished review NO 1200\n",
      "finished review NO 1400\n",
      "finished review NO 1600\n",
      "finished review NO 1800\n",
      "finished review NO 2000\n",
      "finished review NO 2200\n",
      "finished review NO 2400\n",
      "finished review NO 2600\n",
      "finished review NO 2800\n",
      "finished review NO 3000\n",
      "finished review NO 3200\n",
      "finished review NO 3400\n",
      "finished review NO 3600\n",
      "finished review NO 3800\n",
      "finished review NO 4000\n",
      "finished review NO 4200\n",
      "finished review NO 4400\n",
      "finished review NO 4600\n",
      "finished review NO 4800\n",
      "finished review NO 5000\n",
      "finished review NO 5200\n",
      "finished review NO 5400\n",
      "finished review NO 5600\n",
      "finished review NO 5800\n",
      "finished review NO 6000\n",
      "finished review NO 6200\n",
      "finished review NO 6400\n",
      "finished review NO 6600\n",
      "finished review NO 6800\n",
      "finished review NO 7000\n",
      "finished review NO 7200\n",
      "finished review NO 7400\n",
      "finished review NO 7600\n",
      "finished review NO 7800\n",
      "finished review NO 8000\n",
      "finished review NO 8200\n",
      "finished review NO 8400\n",
      "finished review NO 8600\n",
      "finished review NO 8800\n",
      "finished review NO 9000\n",
      "finished review NO 9200\n",
      "finished review NO 9400\n",
      "finished review NO 9600\n",
      "finished review NO 9800\n",
      "finished review NO 10000\n",
      "finished review NO 10200\n",
      "finished review NO 10400\n",
      "finished review NO 10600\n",
      "finished review NO 10800\n",
      "finished review NO 11000\n",
      "finished review NO 11200\n",
      "finished review NO 11400\n",
      "finished review NO 11600\n",
      "finished review NO 11800\n",
      "finished review NO 12000\n",
      "finished review NO 12200\n",
      "finished review NO 12400\n",
      "finished review NO 12600\n",
      "finished review NO 12800\n",
      "finished review NO 13000\n",
      "finished review NO 13200\n",
      "finished review NO 13400\n",
      "finished review NO 13600\n",
      "finished review NO 13800\n",
      "finished review NO 14000\n",
      "finished review NO 14200\n",
      "finished review NO 14400\n",
      "finished review NO 14600\n",
      "finished review NO 14800\n",
      "finished review NO 15000\n",
      "finished review NO 15200\n",
      "finished review NO 15400\n",
      "finished review NO 15600\n",
      "finished review NO 15800\n",
      "finished review NO 16000\n",
      "finished review NO 16200\n",
      "finished review NO 16400\n",
      "finished review NO 16600\n",
      "finished review NO 16800\n",
      "finished review NO 17000\n",
      "finished review NO 17200\n",
      "finished review NO 17400\n",
      "finished review NO 17600\n",
      "finished review NO 17800\n",
      "finished review NO 18000\n",
      "finished review NO 18200\n",
      "finished review NO 18400\n",
      "finished review NO 18600\n",
      "finished review NO 18800\n",
      "finished review NO 19000\n",
      "finished review NO 19200\n",
      "finished review NO 19400\n",
      "finished review NO 19600\n",
      "finished review NO 19800\n",
      "finished review NO 20000\n",
      "The length of train_idx is 20000\n"
     ]
    }
   ],
   "source": [
    "# Convert datasets in token format to index format\n",
    "def convert_token_index(set_token, token2id, id2token):\n",
    "    set_idx = []\n",
    "    count = 0\n",
    "    for review in set_token:\n",
    "        temp_list = []\n",
    "        temp_list = [token2id[token] if token in token2id else UNK_IDX for token in review]\n",
    "        set_idx.append(temp_list)\n",
    "        count += 1\n",
    "        if count % 200 == 0:\n",
    "            print('finished review NO ' + str(count))\n",
    "    return set_idx\n",
    "\n",
    "# Check the train_idx \n",
    "train_idx = convert_token_index(train_tokens, token2id, id2token)\n",
    "print('The length of train_idx is {}'.format(len(train_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished review NO 200\n",
      "finished review NO 400\n",
      "finished review NO 600\n",
      "finished review NO 800\n",
      "finished review NO 1000\n",
      "finished review NO 1200\n",
      "finished review NO 1400\n",
      "finished review NO 1600\n",
      "finished review NO 1800\n",
      "finished review NO 2000\n",
      "finished review NO 2200\n",
      "finished review NO 2400\n",
      "finished review NO 2600\n",
      "finished review NO 2800\n",
      "finished review NO 3000\n",
      "finished review NO 3200\n",
      "finished review NO 3400\n",
      "finished review NO 3600\n",
      "finished review NO 3800\n",
      "finished review NO 4000\n",
      "finished review NO 4200\n",
      "finished review NO 4400\n",
      "finished review NO 4600\n",
      "finished review NO 4800\n",
      "finished review NO 5000\n",
      "The length of val_idx is 5000\n",
      "finished review NO 200\n",
      "finished review NO 400\n",
      "finished review NO 600\n",
      "finished review NO 800\n",
      "finished review NO 1000\n",
      "finished review NO 1200\n",
      "finished review NO 1400\n",
      "finished review NO 1600\n",
      "finished review NO 1800\n",
      "finished review NO 2000\n",
      "finished review NO 2200\n",
      "finished review NO 2400\n",
      "finished review NO 2600\n",
      "finished review NO 2800\n",
      "finished review NO 3000\n",
      "finished review NO 3200\n",
      "finished review NO 3400\n",
      "finished review NO 3600\n",
      "finished review NO 3800\n",
      "finished review NO 4000\n",
      "finished review NO 4200\n",
      "finished review NO 4400\n",
      "finished review NO 4600\n",
      "finished review NO 4800\n",
      "finished review NO 5000\n",
      "finished review NO 5200\n",
      "finished review NO 5400\n",
      "finished review NO 5600\n",
      "finished review NO 5800\n",
      "finished review NO 6000\n",
      "finished review NO 6200\n",
      "finished review NO 6400\n",
      "finished review NO 6600\n",
      "finished review NO 6800\n",
      "finished review NO 7000\n",
      "finished review NO 7200\n",
      "finished review NO 7400\n",
      "finished review NO 7600\n",
      "finished review NO 7800\n",
      "finished review NO 8000\n",
      "finished review NO 8200\n",
      "finished review NO 8400\n",
      "finished review NO 8600\n",
      "finished review NO 8800\n",
      "finished review NO 9000\n",
      "finished review NO 9200\n",
      "finished review NO 9400\n",
      "finished review NO 9600\n",
      "finished review NO 9800\n",
      "finished review NO 10000\n",
      "finished review NO 10200\n",
      "finished review NO 10400\n",
      "finished review NO 10600\n",
      "finished review NO 10800\n",
      "finished review NO 11000\n",
      "finished review NO 11200\n",
      "finished review NO 11400\n",
      "finished review NO 11600\n",
      "finished review NO 11800\n",
      "finished review NO 12000\n",
      "finished review NO 12200\n",
      "finished review NO 12400\n",
      "finished review NO 12600\n",
      "finished review NO 12800\n",
      "finished review NO 13000\n",
      "finished review NO 13200\n",
      "finished review NO 13400\n",
      "finished review NO 13600\n",
      "finished review NO 13800\n",
      "finished review NO 14000\n",
      "finished review NO 14200\n",
      "finished review NO 14400\n",
      "finished review NO 14600\n",
      "finished review NO 14800\n",
      "finished review NO 15000\n",
      "finished review NO 15200\n",
      "finished review NO 15400\n",
      "finished review NO 15600\n",
      "finished review NO 15800\n",
      "finished review NO 16000\n",
      "finished review NO 16200\n",
      "finished review NO 16400\n",
      "finished review NO 16600\n",
      "finished review NO 16800\n",
      "finished review NO 17000\n",
      "finished review NO 17200\n",
      "finished review NO 17400\n",
      "finished review NO 17600\n",
      "finished review NO 17800\n",
      "finished review NO 18000\n",
      "finished review NO 18200\n",
      "finished review NO 18400\n",
      "finished review NO 18600\n",
      "finished review NO 18800\n",
      "finished review NO 19000\n",
      "finished review NO 19200\n",
      "finished review NO 19400\n",
      "finished review NO 19600\n",
      "finished review NO 19800\n",
      "finished review NO 20000\n",
      "finished review NO 20200\n",
      "finished review NO 20400\n",
      "finished review NO 20600\n",
      "finished review NO 20800\n",
      "finished review NO 21000\n",
      "finished review NO 21200\n",
      "finished review NO 21400\n",
      "finished review NO 21600\n",
      "finished review NO 21800\n",
      "finished review NO 22000\n",
      "finished review NO 22200\n",
      "finished review NO 22400\n",
      "finished review NO 22600\n",
      "finished review NO 22800\n",
      "finished review NO 23000\n",
      "finished review NO 23200\n",
      "finished review NO 23400\n",
      "finished review NO 23600\n",
      "finished review NO 23800\n",
      "finished review NO 24000\n",
      "finished review NO 24200\n",
      "finished review NO 24400\n",
      "finished review NO 24600\n",
      "finished review NO 24800\n",
      "finished review NO 25000\n",
      "The length of test_idx is 25000\n"
     ]
    }
   ],
   "source": [
    "# Convert val_tokens and test_tokens to idx sets\n",
    "val_idx = convert_token_index(val_tokens, token2id, id2token)\n",
    "print('The length of val_idx is {}'.format(len(val_idx)))\n",
    "\n",
    "test_idx = convert_token_index(test_tokens, token2id, id2token)\n",
    "print('The length of test_idx is {}'.format(len(test_idx)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Establishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.1021\n"
     ]
    }
   ],
   "source": [
    "# Find out average sentence lengths\n",
    "total = 0\n",
    "for review in train_idx:\n",
    "    total += len(review)\n",
    "print(float(total) / len(train_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up maximum sentence length for the use of standardizing matirx format later in the program\n",
    "MAX_SENTENCE_LENGTH = 500\n",
    "\n",
    "# Set up local class of dataset to use in this program \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class localDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_idx, label_list):\n",
    "        self.data_idx = data_idx\n",
    "        self.label_list = label_list\n",
    "        assert(len(self.data_idx) == len(self.label_list))\n",
    "    def __len__(self):\n",
    "        return len(self.data_idx)\n",
    "    def __getitem__(self, key):\n",
    "        review_idx = self.data_idx[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.label_list[key]\n",
    "        return [review_idx, len(review_idx), label]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    idx_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    for review in batch:\n",
    "        label_list.append(review[2])\n",
    "        length_list.append(review[1])\n",
    "        padded_review = np.pad(np.array(review[0]),pad_width=((0,MAX_SENTENCE_LENGTH-review[1])),mode=\"constant\", constant_values=0)\n",
    "        idx_list.append(padded_review)\n",
    "    return [torch.from_numpy(np.array(idx_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dataset\n",
    "train_dataset = localDataset(train_idx, train_label)\n",
    "val_dataset = localDataset(val_idx, val_label)\n",
    "test_dataset = localDataset(test_idx, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "BATCH_SIZE = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIER_MODE = 'score'\n",
    "# Establish the BagOfWords Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    def __init__(self, review_length, embed_dim):\n",
    "        super(BagOfWords, self).__init__()\n",
    "        self.embedding = nn.Embedding(review_length, embed_dim, padding_idx = 0)\n",
    "        self.linearReg1 = nn.Linear(embed_dim, 50)\n",
    "        self.linearReg2 = nn.Linear(50, 11)\n",
    "    def forward(self, idx_list, length_list):\n",
    "        embed_result_words = self.embedding(idx_list)\n",
    "        embed_result_review = torch.sum(embed_result_words, dim = 1)\n",
    "        embed_result = embed_result_review / length_list.view(length_list.size()[0],1).expand_as(embed_result_review).float()\n",
    "        \n",
    "        result = F.relu(self.linearReg1(embed_result.float()))\n",
    "        result = self.linearReg2(result.float())\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without Learning Rate Annealing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BagOfWords(len(id2token), EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step: [101/313], Validation Acc: 36.54\n",
      "Epoch: [1/3], Step: [201/313], Validation Acc: 38.32\n",
      "Epoch: [1/3], Step: [301/313], Validation Acc: 39.18\n",
      "Epoch: [2/3], Step: [101/313], Validation Acc: 38.38\n",
      "Epoch: [2/3], Step: [201/313], Validation Acc: 38.24\n",
      "Epoch: [2/3], Step: [301/313], Validation Acc: 37.74\n",
      "Epoch: [3/3], Step: [101/313], Validation Acc: 36.96\n",
      "Epoch: [3/3], Step: [201/313], Validation Acc: 33.02\n",
      "Epoch: [3/3], Step: [301/313], Validation Acc: 36.14\n",
      "Final Validation Acc: 36.14\n"
     ]
    }
   ],
   "source": [
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "if OPTIMIZER == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, NUM_EPOCHS, i+1, len(train_loader), val_acc))\n",
    "\n",
    "final_accuracy = test_model(val_loader, model)\n",
    "print('Final Validation Acc: {}'.format( val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With Learning Rate Annealing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BagOfWords(len(id2token), EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/3], Step: [101/313], Validation Acc: 37.64\n",
      "Epoch: [1/3], Step: [201/313], Validation Acc: 37.48\n",
      "Epoch: [1/3], Step: [301/313], Validation Acc: 37.06\n",
      "Epoch: [2/3], Step: [101/313], Validation Acc: 36.5\n",
      "Epoch: [2/3], Step: [201/313], Validation Acc: 36.18\n",
      "Epoch: [2/3], Step: [301/313], Validation Acc: 35.78\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-226-360a32dba06a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# validate every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpclass/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "lambda1 = lambda epoch: 0.3**epoch\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "if OPTIMIZER == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    \n",
    "scheduler = LambdaLR(optimizer, lr_lambda=[lambda1])\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        #scheduler.step()\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, NUM_EPOCHS, i+1, len(train_loader), val_acc))\n",
    "\n",
    "final_accuracy = test_model(val_loader, model)\n",
    "print('Final Validation Acc: {}'.format(final_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
