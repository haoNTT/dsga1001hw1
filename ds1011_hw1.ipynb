{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS-GA 1011 Assignment 1\n",
    "## Haonan Tian ht1151@nyu.edu \n",
    "## 10/02/2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import spacy\n",
    "import string\n",
    "import random\n",
    "import torch\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "TOKEN_MODE = 'Plain'\n",
    "NGRAM = 1\n",
    "MAX_VOCAB_SIZE = 20000  # Set maximum vocabulary size\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 4\n",
    "EMBED_DIM = 100\n",
    "OPTIMIZER = 'ADAM'\n",
    "LAERNING_ANNEALING = 'YES'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize path to the directory where the files are stored\n",
    "path_train_pos = '/train/pos'\n",
    "path_train_neg = '/train/neg'\n",
    "path_test_pos = '/test/pos'\n",
    "path_test_neg = '/test/neg'\n",
    "current_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the loaded files are train_pos 12500 train_neg 12500 test_pos 12500 test_neg 12500\n"
     ]
    }
   ],
   "source": [
    "# Load files\n",
    "train_pos = []\n",
    "train_neg = []\n",
    "test_pos = []\n",
    "test_neg = []\n",
    "os.chdir(current_path+path_train_pos)\n",
    "for file in os.listdir():\n",
    "    fin = open(file, 'r')\n",
    "    train_pos.append(fin.read())\n",
    "    fin.close()\n",
    "\n",
    "os.chdir(current_path+path_train_neg)\n",
    "for file in os.listdir():\n",
    "    fin = open(file, 'r')\n",
    "    train_neg.append(fin.read())\n",
    "    fin.close()\n",
    "\n",
    "os.chdir(current_path+path_test_pos)\n",
    "for file in os.listdir():\n",
    "    fin = open(file, 'r')\n",
    "    test_pos.append(fin.read())\n",
    "    fin.close()\n",
    "\n",
    "os.chdir(current_path+path_test_neg)\n",
    "for file in os.listdir():\n",
    "    fin = open(file, 'r')\n",
    "    test_neg.append(fin.read())\n",
    "    fin.close()\n",
    "    \n",
    "# Check of the test has been correctly loaded\n",
    "print('The length of the loaded files are train_pos {} train_neg {} test_pos {} test_neg {}'.format(len(train_pos),len(train_neg),len(test_pos),len(test_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the loaded files are train_pos 10000 train_neg 10000 val_pos 2500 val_neg 2500\n"
     ]
    }
   ],
   "source": [
    "# Split the training set to training set and validation set\n",
    "TRAIN_VAL_SPLIT = 10000\n",
    "val_pos = []\n",
    "val_neg = []\n",
    "val_pos = train_pos[TRAIN_VAL_SPLIT:]\n",
    "train_pos = train_pos[:TRAIN_VAL_SPLIT]\n",
    "val_neg = train_neg[TRAIN_VAL_SPLIT:]\n",
    "train_neg = train_neg[:TRAIN_VAL_SPLIT]\n",
    "\n",
    "# Check the correctness of the split\n",
    "print('The length of the loaded files are train_pos {} train_neg {} val_pos {} val_neg {}'.format(len(train_pos),len(train_neg),len(val_pos),len(val_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine positive and negative datasets\n",
    "train_set = train_pos + train_neg\n",
    "val_set = val_pos + val_neg\n",
    "test_set = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plain Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tokenizer and filter to remove puntuations and standardize the tokens\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Set up tokenizer\n",
    "def tokenize(inputStr):\n",
    "    tokens = tokenizer(inputStr)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "\n",
    "# Check the correctness\n",
    "token_output = tokenize('This is A test for the preprocessing part, !!! to see if the tokenization works correctly!!!')\n",
    "print(token_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize for Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "The length of train_tokens and all_tokens are 20000 4808696\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all datasets\n",
    "def tokenize_dataset(input_set):\n",
    "    all_tokens = []\n",
    "    set_tokens = []\n",
    "    inner_counter = 1\n",
    "    for review in input_set:\n",
    "        token_temp = tokenize(review)\n",
    "        all_tokens += token_temp\n",
    "        set_tokens.append(token_temp)\n",
    "        if inner_counter % 500 == 0:\n",
    "            print('Finished tokenizing review {}'.format(inner_counter))\n",
    "        inner_counter += 1\n",
    "    return set_tokens, all_tokens\n",
    "\n",
    "# Check the output of tokenization sets\n",
    "train_tokens, all_tokens = tokenize_dataset(train_set)\n",
    "print('The length of train_tokens and all_tokens are {} {}'.format(len(train_tokens),len(all_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "The length of val_tokens is 5000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 20500\n",
      "Finished tokenizing review 21000\n",
      "Finished tokenizing review 21500\n",
      "Finished tokenizing review 22000\n",
      "Finished tokenizing review 22500\n",
      "Finished tokenizing review 23000\n",
      "Finished tokenizing review 23500\n",
      "Finished tokenizing review 24000\n",
      "Finished tokenizing review 24500\n",
      "Finished tokenizing review 25000\n",
      "The length of test_tokens is 25000\n"
     ]
    }
   ],
   "source": [
    "# Perform tokenization for val_set and test_set\n",
    "val_tokens, _ = tokenize_dataset(val_set)\n",
    "print('The length of val_tokens is {}'.format(len(val_tokens)))\n",
    "\n",
    "test_tokens, _ = tokenize_dataset(test_set)\n",
    "print('The length of test_tokens is {}'.format(len(test_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the three tokenized datasets into pickle files\n",
    "os.chdir(current_path)\n",
    "pkl.dump(train_tokens, open(\"train_tokens.p\", \"wb\"))\n",
    "pkl.dump(val_tokens, open(\"val_tokens.p\", \"wb\"))\n",
    "pkl.dump(test_tokens, open(\"test_tokens.p\", \"wb\"))\n",
    "\n",
    "# Store all tokens appered in training set into a pickle file \n",
    "pkl.dump(all_tokens, open(\"all_tokens.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set for n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is a test', 'is a test for', 'a test for the', 'test for the preprocessing', 'for the preprocessing part', 'the preprocessing part to', 'preprocessing part to see', 'part to see if', 'to see if the', 'see if the tokenization', 'if the tokenization works', 'the tokenization works correctly']\n"
     ]
    }
   ],
   "source": [
    "# Create function to tokenize for n-grams\n",
    "from spacy.tokens.token import Token\n",
    "def tokenize_ngram(inputStr, N):\n",
    "    token_results = []\n",
    "    tokens = tokenizer(inputStr)\n",
    "    tokens = [token.text.lower() for token in tokens if (token.text not in punctuations)]\n",
    "    counter = 0\n",
    "    while counter < len(tokens)-N+1:\n",
    "        temp = ''\n",
    "        for i in range(N):\n",
    "            temp += str(tokens[counter+i]) + ' '\n",
    "        token_results.append(temp.strip(' '))\n",
    "        \n",
    "        counter += 1\n",
    "    return token_results\n",
    "\n",
    "# Check the correctness\n",
    "token_output = tokenize_ngram('This is A test for the preprocessing part, !!! to see if the tokenization works correctly!!!',NGRAM)\n",
    "print(token_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all datasets for ngram\n",
    "def tokenize_dataset_ngram(input_set, N):\n",
    "    all_tokens = []\n",
    "    set_tokens = []\n",
    "    inner_counter = 1\n",
    "    for review in input_set:\n",
    "        token_temp = tokenize_ngram(review, N)\n",
    "        all_tokens += token_temp\n",
    "        set_tokens.append(token_temp)\n",
    "        if inner_counter % 500 == 0:\n",
    "            print('Finished tokenizing review {}'.format(inner_counter))\n",
    "        inner_counter += 1\n",
    "    return set_tokens, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 20500\n",
      "Finished tokenizing review 21000\n",
      "Finished tokenizing review 21500\n",
      "Finished tokenizing review 22000\n",
      "Finished tokenizing review 22500\n",
      "Finished tokenizing review 23000\n",
      "Finished tokenizing review 23500\n",
      "Finished tokenizing review 24000\n",
      "Finished tokenizing review 24500\n",
      "Finished tokenizing review 25000\n",
      "\n",
      "Finished 2gram\n",
      "\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 20500\n",
      "Finished tokenizing review 21000\n",
      "Finished tokenizing review 21500\n",
      "Finished tokenizing review 22000\n",
      "Finished tokenizing review 22500\n",
      "Finished tokenizing review 23000\n",
      "Finished tokenizing review 23500\n",
      "Finished tokenizing review 24000\n",
      "Finished tokenizing review 24500\n",
      "Finished tokenizing review 25000\n",
      "\n",
      "Finished 3gram\n",
      "\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n",
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 500\n",
      "Finished tokenizing review 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing review 1500\n",
      "Finished tokenizing review 2000\n",
      "Finished tokenizing review 2500\n",
      "Finished tokenizing review 3000\n",
      "Finished tokenizing review 3500\n",
      "Finished tokenizing review 4000\n",
      "Finished tokenizing review 4500\n",
      "Finished tokenizing review 5000\n",
      "Finished tokenizing review 5500\n",
      "Finished tokenizing review 6000\n",
      "Finished tokenizing review 6500\n",
      "Finished tokenizing review 7000\n",
      "Finished tokenizing review 7500\n",
      "Finished tokenizing review 8000\n",
      "Finished tokenizing review 8500\n",
      "Finished tokenizing review 9000\n",
      "Finished tokenizing review 9500\n",
      "Finished tokenizing review 10000\n",
      "Finished tokenizing review 10500\n",
      "Finished tokenizing review 11000\n",
      "Finished tokenizing review 11500\n",
      "Finished tokenizing review 12000\n",
      "Finished tokenizing review 12500\n",
      "Finished tokenizing review 13000\n",
      "Finished tokenizing review 13500\n",
      "Finished tokenizing review 14000\n",
      "Finished tokenizing review 14500\n",
      "Finished tokenizing review 15000\n",
      "Finished tokenizing review 15500\n",
      "Finished tokenizing review 16000\n",
      "Finished tokenizing review 16500\n",
      "Finished tokenizing review 17000\n",
      "Finished tokenizing review 17500\n",
      "Finished tokenizing review 18000\n",
      "Finished tokenizing review 18500\n",
      "Finished tokenizing review 19000\n",
      "Finished tokenizing review 19500\n",
      "Finished tokenizing review 20000\n",
      "Finished tokenizing review 20500\n",
      "Finished tokenizing review 21000\n",
      "Finished tokenizing review 21500\n",
      "Finished tokenizing review 22000\n",
      "Finished tokenizing review 22500\n",
      "Finished tokenizing review 23000\n",
      "Finished tokenizing review 23500\n",
      "Finished tokenizing review 24000\n",
      "Finished tokenizing review 24500\n",
      "Finished tokenizing review 25000\n",
      "\n",
      "Finished 4gram\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create tokens for N-grams N > 1\n",
    "# NOTE: This may take more than an hour to run\n",
    "N_GRAM = [2,3,4]\n",
    "for N in N_GRAM:\n",
    "    train_tokens, all_tokens = tokenize_dataset_ngram(train_set, N)\n",
    "    val_tokens, _ = tokenize_dataset_ngram(val_set, N)\n",
    "    test_tokens, _ = tokenize_dataset_ngram(test_set, N)\n",
    "    os.chdir(current_path)\n",
    "    pkl.dump(train_tokens, open(\"train_tokens_\" + str(N) + \".p\", \"wb\"))\n",
    "    pkl.dump(val_tokens, open(\"val_tokens_\" + str(N) + \".p\", \"wb\"))\n",
    "    pkl.dump(test_tokens, open(\"test_tokens_\" + str(N) + \".p\", \"wb\"))\n",
    "    pkl.dump(all_tokens, open(\"all_tokens_\" + str(N) + \".p\", \"wb\"))\n",
    "    print('\\nFinished ' + str(N) + \"gram\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stored N-Gram pickle and load into program\n",
    "def extract_n_gram_tokens(N):\n",
    "    os.chdir(current_path)\n",
    "    train_tokens = pkl.load(open(current_path + \"/train_tokens_\" + str(N) + \".p\", \"rb\"))\n",
    "    val_tokens = pkl.load(open(current_path + \"/val_tokens_\" + str(N) + \".p\", \"rb\"))\n",
    "    test_tokens = pkl.load(open(current_path + \"/test_tokens_\" + str(N) + \".p\", \"rb\"))\n",
    "    all_tokens = pkl.load(open(current_path + \"/all_tokens_\" + str(N) + \".p\", \"rb\"))\n",
    "    return train_tokens, val_tokens, test_tokens, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 1-gram token sets\n",
    "train_tokens = pkl.load(open(current_path + \"/train_tokens.p\", \"rb\"))\n",
    "val_tokens = pkl.load(open(current_path + \"/val_tokens.p\", \"rb\"))\n",
    "test_tokens = pkl.load(open(current_path + \"/test_tokens.p\", \"rb\"))\n",
    "all_tokens = pkl.load(open(current_path + \"/all_tokens.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 2-gram token sets\n",
    "train_tokens, val_tokens, test_tokens, all_tokens = extract_n_gram_tokens(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 3-gram token sets\n",
    "train_tokens, val_tokens, test_tokens, all_tokens = extract_n_gram_tokens(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 4-gram token sets\n",
    "train_tokens, val_tokens, test_tokens, all_tokens = extract_n_gram_tokens(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 261965), ('and', 130352), ('a', 128951), ('of', 116768), ('to', 107941), ('is', 87781), ('it', 74745), ('in', 74062), ('i', 66302), ('this', 58666)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(all_tokens).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations, stopwords and perform stemming\n",
    "punctuations = string.punctuation\n",
    "stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def tokenize_sentence_advanced(inputStr, N):\n",
    "    token_results = []\n",
    "    tokens = word_tokenize(inputStr)\n",
    "    tokens = [ps.stem(token.lower()) for token in tokens if (token not in punctuations and token not in stopwords)]\n",
    "    if N == 1:\n",
    "        return tokens\n",
    "    else:\n",
    "        counter = 0\n",
    "        while counter < len(tokens)-N+1:\n",
    "            temp = ''\n",
    "            for i in range(N):\n",
    "                temp += str(tokens[counter+i]) + ' '\n",
    "            token_results.append(temp.strip(' '))\n",
    "            counter += 1\n",
    "        return token_results\n",
    "# Tokenize all datasets for ngram\n",
    "def tokenize_dataset_ngram(input_set, N):\n",
    "    all_tokens = []\n",
    "    set_tokens = []\n",
    "    inner_counter = 1\n",
    "    for review in input_set:\n",
    "        token_temp = tokenize_sentence_advanced(review, N)\n",
    "        all_tokens += token_temp\n",
    "        set_tokens.append(token_temp)\n",
    "        if inner_counter % 500 == 0:\n",
    "            print('Finished tokenizing review {}'.format(inner_counter))\n",
    "        inner_counter += 1\n",
    "    return set_tokens, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ngram advanced tokenization\n",
    "for N in range(4):\n",
    "    train_tokens, all_tokens = tokenize_dataset_ngram(train_set, N+1)\n",
    "    val_tokens, _ = tokenize_dataset_ngram(val_set, N+1)\n",
    "    test_tokens, _ = tokenize_dataset_ngram(test_set, N+1)\n",
    "    os.chdir(current_path)\n",
    "    pkl.dump(train_tokens, open(\"train_tokens_\" + str(N+1) + '_ad' + \".p\", \"wb\"))\n",
    "    pkl.dump(val_tokens, open(\"val_tokens_\" + str(N+1) + '_ad' + \".p\", \"wb\"))\n",
    "    pkl.dump(test_tokens, open(\"test_tokens_\" + str(N+1) + '_ad' + \".p\", \"wb\"))\n",
    "    pkl.dump(all_tokens, open(\"all_tokens_\" + str(N+1) + '_ad' + \".p\", \"wb\"))\n",
    "    print('\\nFinished ' + str(N+1) + \"gram\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract stored N-Gram pickle and load into program\n",
    "def extract_n_gram_tokens_ad(N):\n",
    "    os.chdir(current_path)\n",
    "    train_tokens = pkl.load(open(current_path + \"/train_tokens_\" + str(N) + '_ad' + \".p\", \"rb\"))\n",
    "    val_tokens = pkl.load(open(current_path + \"/val_tokens_\" + str(N) + '_ad' + \".p\", \"rb\"))\n",
    "    test_tokens = pkl.load(open(current_path + \"/test_tokens_\" + str(N) + '_ad' + \".p\", \"rb\"))\n",
    "    all_tokens = pkl.load(open(current_path + \"/all_tokens_\" + str(N) + '_ad' + \".p\", \"rb\"))\n",
    "    return train_tokens, val_tokens, test_tokens, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 1-gram advanced token sets\n",
    "train_tokens, val_tokens, test_tokens, all_tokens = extract_n_gram_tokens_ad(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 2-gram advanced token sets\n",
    "train_tokens, val_tokens, test_tokens, all_tokens = extract_n_gram_tokens_ad(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 3-gram advanced token sets\n",
    "train_tokens, val_tokens, test_tokens, all_tokens = extract_n_gram_tokens_ad(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 4-gram advanced token sets\n",
    "train_tokens, val_tokens, test_tokens, all_tokens = extract_n_gram_tokens_ad(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('br br the film', 560), (\"br br it 's\", 535), ('br br thi movi', 383), (\"i 've ever seen\", 371), ('br br the movi', 356), ('br br the stori', 293), ('br br thi film', 287), ('br br the plot', 234), ('br br the act', 209), (\"br br i n't\", 203)]\n"
     ]
    }
   ],
   "source": [
    "print(Counter(all_tokens).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserve special index for padding and unknown words\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "# The method to build vocabulary\n",
    "def build_vocab(all_tokens):\n",
    "    token_counter = Counter(all_tokens)\n",
    "    tokens, frequency = zip(*token_counter.most_common(MAX_VOCAB_SIZE))\n",
    "    id2token = list(tokens)\n",
    "    token2id = dict(zip(tokens, range(2,2+len(tokens))))\n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return id2token, token2id\n",
    "\n",
    "id2token, token2id = build_vocab(all_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20002\n"
     ]
    }
   ],
   "source": [
    "print(len(id2token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 4522 ; token liberti\n",
      "Token liberti; token id 4522\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished review NO 200\n",
      "finished review NO 400\n",
      "finished review NO 600\n",
      "finished review NO 800\n",
      "finished review NO 1000\n",
      "finished review NO 1200\n",
      "finished review NO 1400\n",
      "finished review NO 1600\n",
      "finished review NO 1800\n",
      "finished review NO 2000\n",
      "finished review NO 2200\n",
      "finished review NO 2400\n",
      "finished review NO 2600\n",
      "finished review NO 2800\n",
      "finished review NO 3000\n",
      "finished review NO 3200\n",
      "finished review NO 3400\n",
      "finished review NO 3600\n",
      "finished review NO 3800\n",
      "finished review NO 4000\n",
      "finished review NO 4200\n",
      "finished review NO 4400\n",
      "finished review NO 4600\n",
      "finished review NO 4800\n",
      "finished review NO 5000\n",
      "finished review NO 5200\n",
      "finished review NO 5400\n",
      "finished review NO 5600\n",
      "finished review NO 5800\n",
      "finished review NO 6000\n",
      "finished review NO 6200\n",
      "finished review NO 6400\n",
      "finished review NO 6600\n",
      "finished review NO 6800\n",
      "finished review NO 7000\n",
      "finished review NO 7200\n",
      "finished review NO 7400\n",
      "finished review NO 7600\n",
      "finished review NO 7800\n",
      "finished review NO 8000\n",
      "finished review NO 8200\n",
      "finished review NO 8400\n",
      "finished review NO 8600\n",
      "finished review NO 8800\n",
      "finished review NO 9000\n",
      "finished review NO 9200\n",
      "finished review NO 9400\n",
      "finished review NO 9600\n",
      "finished review NO 9800\n",
      "finished review NO 10000\n",
      "finished review NO 10200\n",
      "finished review NO 10400\n",
      "finished review NO 10600\n",
      "finished review NO 10800\n",
      "finished review NO 11000\n",
      "finished review NO 11200\n",
      "finished review NO 11400\n",
      "finished review NO 11600\n",
      "finished review NO 11800\n",
      "finished review NO 12000\n",
      "finished review NO 12200\n",
      "finished review NO 12400\n",
      "finished review NO 12600\n",
      "finished review NO 12800\n",
      "finished review NO 13000\n",
      "finished review NO 13200\n",
      "finished review NO 13400\n",
      "finished review NO 13600\n",
      "finished review NO 13800\n",
      "finished review NO 14000\n",
      "finished review NO 14200\n",
      "finished review NO 14400\n",
      "finished review NO 14600\n",
      "finished review NO 14800\n",
      "finished review NO 15000\n",
      "finished review NO 15200\n",
      "finished review NO 15400\n",
      "finished review NO 15600\n",
      "finished review NO 15800\n",
      "finished review NO 16000\n",
      "finished review NO 16200\n",
      "finished review NO 16400\n",
      "finished review NO 16600\n",
      "finished review NO 16800\n",
      "finished review NO 17000\n",
      "finished review NO 17200\n",
      "finished review NO 17400\n",
      "finished review NO 17600\n",
      "finished review NO 17800\n",
      "finished review NO 18000\n",
      "finished review NO 18200\n",
      "finished review NO 18400\n",
      "finished review NO 18600\n",
      "finished review NO 18800\n",
      "finished review NO 19000\n",
      "finished review NO 19200\n",
      "finished review NO 19400\n",
      "finished review NO 19600\n",
      "finished review NO 19800\n",
      "finished review NO 20000\n",
      "The length of train_idx is 20000\n"
     ]
    }
   ],
   "source": [
    "# Convert datasets in token format to index format\n",
    "def convert_token_index(set_token, token2id, id2token):\n",
    "    set_idx = []\n",
    "    count = 0\n",
    "    for review in set_token:\n",
    "        temp_list = []\n",
    "        temp_list = [token2id[token] if token in token2id else UNK_IDX for token in review]\n",
    "        set_idx.append(temp_list)\n",
    "        count += 1\n",
    "        if count % 200 == 0:\n",
    "            print('finished review NO ' + str(count))\n",
    "    return set_idx\n",
    "\n",
    "# Check the train_idx \n",
    "train_idx = convert_token_index(train_tokens, token2id, id2token)\n",
    "print('The length of train_idx is {}'.format(len(train_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished review NO 200\n",
      "finished review NO 400\n",
      "finished review NO 600\n",
      "finished review NO 800\n",
      "finished review NO 1000\n",
      "finished review NO 1200\n",
      "finished review NO 1400\n",
      "finished review NO 1600\n",
      "finished review NO 1800\n",
      "finished review NO 2000\n",
      "finished review NO 2200\n",
      "finished review NO 2400\n",
      "finished review NO 2600\n",
      "finished review NO 2800\n",
      "finished review NO 3000\n",
      "finished review NO 3200\n",
      "finished review NO 3400\n",
      "finished review NO 3600\n",
      "finished review NO 3800\n",
      "finished review NO 4000\n",
      "finished review NO 4200\n",
      "finished review NO 4400\n",
      "finished review NO 4600\n",
      "finished review NO 4800\n",
      "finished review NO 5000\n",
      "The length of val_idx is 5000\n",
      "finished review NO 200\n",
      "finished review NO 400\n",
      "finished review NO 600\n",
      "finished review NO 800\n",
      "finished review NO 1000\n",
      "finished review NO 1200\n",
      "finished review NO 1400\n",
      "finished review NO 1600\n",
      "finished review NO 1800\n",
      "finished review NO 2000\n",
      "finished review NO 2200\n",
      "finished review NO 2400\n",
      "finished review NO 2600\n",
      "finished review NO 2800\n",
      "finished review NO 3000\n",
      "finished review NO 3200\n",
      "finished review NO 3400\n",
      "finished review NO 3600\n",
      "finished review NO 3800\n",
      "finished review NO 4000\n",
      "finished review NO 4200\n",
      "finished review NO 4400\n",
      "finished review NO 4600\n",
      "finished review NO 4800\n",
      "finished review NO 5000\n",
      "finished review NO 5200\n",
      "finished review NO 5400\n",
      "finished review NO 5600\n",
      "finished review NO 5800\n",
      "finished review NO 6000\n",
      "finished review NO 6200\n",
      "finished review NO 6400\n",
      "finished review NO 6600\n",
      "finished review NO 6800\n",
      "finished review NO 7000\n",
      "finished review NO 7200\n",
      "finished review NO 7400\n",
      "finished review NO 7600\n",
      "finished review NO 7800\n",
      "finished review NO 8000\n",
      "finished review NO 8200\n",
      "finished review NO 8400\n",
      "finished review NO 8600\n",
      "finished review NO 8800\n",
      "finished review NO 9000\n",
      "finished review NO 9200\n",
      "finished review NO 9400\n",
      "finished review NO 9600\n",
      "finished review NO 9800\n",
      "finished review NO 10000\n",
      "finished review NO 10200\n",
      "finished review NO 10400\n",
      "finished review NO 10600\n",
      "finished review NO 10800\n",
      "finished review NO 11000\n",
      "finished review NO 11200\n",
      "finished review NO 11400\n",
      "finished review NO 11600\n",
      "finished review NO 11800\n",
      "finished review NO 12000\n",
      "finished review NO 12200\n",
      "finished review NO 12400\n",
      "finished review NO 12600\n",
      "finished review NO 12800\n",
      "finished review NO 13000\n",
      "finished review NO 13200\n",
      "finished review NO 13400\n",
      "finished review NO 13600\n",
      "finished review NO 13800\n",
      "finished review NO 14000\n",
      "finished review NO 14200\n",
      "finished review NO 14400\n",
      "finished review NO 14600\n",
      "finished review NO 14800\n",
      "finished review NO 15000\n",
      "finished review NO 15200\n",
      "finished review NO 15400\n",
      "finished review NO 15600\n",
      "finished review NO 15800\n",
      "finished review NO 16000\n",
      "finished review NO 16200\n",
      "finished review NO 16400\n",
      "finished review NO 16600\n",
      "finished review NO 16800\n",
      "finished review NO 17000\n",
      "finished review NO 17200\n",
      "finished review NO 17400\n",
      "finished review NO 17600\n",
      "finished review NO 17800\n",
      "finished review NO 18000\n",
      "finished review NO 18200\n",
      "finished review NO 18400\n",
      "finished review NO 18600\n",
      "finished review NO 18800\n",
      "finished review NO 19000\n",
      "finished review NO 19200\n",
      "finished review NO 19400\n",
      "finished review NO 19600\n",
      "finished review NO 19800\n",
      "finished review NO 20000\n",
      "finished review NO 20200\n",
      "finished review NO 20400\n",
      "finished review NO 20600\n",
      "finished review NO 20800\n",
      "finished review NO 21000\n",
      "finished review NO 21200\n",
      "finished review NO 21400\n",
      "finished review NO 21600\n",
      "finished review NO 21800\n",
      "finished review NO 22000\n",
      "finished review NO 22200\n",
      "finished review NO 22400\n",
      "finished review NO 22600\n",
      "finished review NO 22800\n",
      "finished review NO 23000\n",
      "finished review NO 23200\n",
      "finished review NO 23400\n",
      "finished review NO 23600\n",
      "finished review NO 23800\n",
      "finished review NO 24000\n",
      "finished review NO 24200\n",
      "finished review NO 24400\n",
      "finished review NO 24600\n",
      "finished review NO 24800\n",
      "finished review NO 25000\n",
      "The length of test_idx is 25000\n"
     ]
    }
   ],
   "source": [
    "# Convert val_tokens and test_tokens to idx sets\n",
    "val_idx = convert_token_index(val_tokens, token2id, id2token)\n",
    "print('The length of val_idx is {}'.format(len(val_idx)))\n",
    "\n",
    "test_idx = convert_token_index(test_tokens, token2id, id2token)\n",
    "print('The length of test_idx is {}'.format(len(test_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2558,\n",
       " 1,\n",
       " 678,\n",
       " 13090,\n",
       " 570,\n",
       " 849,\n",
       " 63,\n",
       " 61,\n",
       " 698,\n",
       " 189,\n",
       " 621,\n",
       " 367,\n",
       " 257,\n",
       " 235,\n",
       " 35,\n",
       " 366,\n",
       " 52,\n",
       " 1433,\n",
       " 448,\n",
       " 829,\n",
       " 33,\n",
       " 39,\n",
       " 2558,\n",
       " 406,\n",
       " 591,\n",
       " 1621,\n",
       " 46,\n",
       " 2809,\n",
       " 253,\n",
       " 3748,\n",
       " 3243,\n",
       " 172,\n",
       " 1822,\n",
       " 1721,\n",
       " 2826,\n",
       " 9404,\n",
       " 1621,\n",
       " 181,\n",
       " 348,\n",
       " 37,\n",
       " 17022,\n",
       " 2,\n",
       " 2,\n",
       " 49,\n",
       " 315,\n",
       " 1789,\n",
       " 96,\n",
       " 621,\n",
       " 1258,\n",
       " 146,\n",
       " 5088,\n",
       " 319,\n",
       " 197,\n",
       " 109,\n",
       " 3308,\n",
       " 296,\n",
       " 1156,\n",
       " 1204,\n",
       " 235,\n",
       " 1998,\n",
       " 19,\n",
       " 4,\n",
       " 12,\n",
       " 2558,\n",
       " 155,\n",
       " 17023,\n",
       " 6172,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 3388,\n",
       " 2041,\n",
       " 118,\n",
       " 106,\n",
       " 6172,\n",
       " 42,\n",
       " 957,\n",
       " 73,\n",
       " 235,\n",
       " 113,\n",
       " 341,\n",
       " 17,\n",
       " 1789,\n",
       " 3508,\n",
       " 2218,\n",
       " 10452,\n",
       " 13843,\n",
       " 19,\n",
       " 96,\n",
       " 621,\n",
       " 2495,\n",
       " 116,\n",
       " 146,\n",
       " 5088,\n",
       " 6172,\n",
       " 2483,\n",
       " 57,\n",
       " 685,\n",
       " 867,\n",
       " 17,\n",
       " 640,\n",
       " 7,\n",
       " 1789,\n",
       " 4,\n",
       " 6172,\n",
       " 1343,\n",
       " 621,\n",
       " 13091,\n",
       " 1932,\n",
       " 5028,\n",
       " 108,\n",
       " 223,\n",
       " 125,\n",
       " 9,\n",
       " 1038,\n",
       " 4516,\n",
       " 78,\n",
       " 4,\n",
       " 315,\n",
       " 7716,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 405,\n",
       " 9744,\n",
       " 6172,\n",
       " 374,\n",
       " 20,\n",
       " 281,\n",
       " 188,\n",
       " 185,\n",
       " 3782,\n",
       " 13092,\n",
       " 1064,\n",
       " 3022,\n",
       " 1,\n",
       " 119,\n",
       " 2576,\n",
       " 746,\n",
       " 319,\n",
       " 3309,\n",
       " 3885,\n",
       " 1790,\n",
       " 3783,\n",
       " 14638,\n",
       " 8130,\n",
       " 2112,\n",
       " 442,\n",
       " 82,\n",
       " 621,\n",
       " 158,\n",
       " 117,\n",
       " 2396,\n",
       " 6172,\n",
       " 9,\n",
       " 78,\n",
       " 4,\n",
       " 82,\n",
       " 990,\n",
       " 4738,\n",
       " 9405,\n",
       " 12,\n",
       " 957,\n",
       " 4,\n",
       " 300,\n",
       " 2368,\n",
       " 122,\n",
       " 13093,\n",
       " 2,\n",
       " 2,\n",
       " 426,\n",
       " 41,\n",
       " 931,\n",
       " 3782,\n",
       " 6172,\n",
       " 9,\n",
       " 1582,\n",
       " 54,\n",
       " 3,\n",
       " 211,\n",
       " 10,\n",
       " 66,\n",
       " 2685,\n",
       " 8,\n",
       " 11,\n",
       " 3388,\n",
       " 2041,\n",
       " 1759,\n",
       " 6,\n",
       " 2206,\n",
       " 132,\n",
       " 28,\n",
       " 3068,\n",
       " 439,\n",
       " 545,\n",
       " 2352,\n",
       " 52,\n",
       " 5799,\n",
       " 5977,\n",
       " 120,\n",
       " 4082,\n",
       " 13844,\n",
       " 448,\n",
       " 28,\n",
       " 4,\n",
       " 12,\n",
       " 86,\n",
       " 3634,\n",
       " 746,\n",
       " 348,\n",
       " 116,\n",
       " 299,\n",
       " 17,\n",
       " 331,\n",
       " 1789,\n",
       " 12,\n",
       " 957,\n",
       " 33,\n",
       " 9,\n",
       " 47,\n",
       " 229,\n",
       " 227,\n",
       " 71,\n",
       " 2558,\n",
       " 263,\n",
       " 82,\n",
       " 12,\n",
       " 11876,\n",
       " 6763,\n",
       " 2,\n",
       " 2,\n",
       " 868,\n",
       " 227,\n",
       " 6,\n",
       " 779,\n",
       " 189,\n",
       " 297]"
      ]
     },
     "execution_count": 960,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0]\n",
    "train_idx[1]\n",
    "#len(Counter(all_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_idx_set(train_idx, val_idx, test_idx, token_mode, n_gram, vocab_size, optimizer, current_path):\n",
    "    os.chdir(current_path)\n",
    "    pkl.dump(train_idx, open(\"train_idx_\" + token_mode + ' ' + str(n_gram) + optimizer + ' ' + str(vocab_size) + \".p\", \"wb\"))\n",
    "    pkl.dump(val_idx, open(\"val_idx_\" + token_mode + ' ' + str(n_gram) + optimizer + ' ' + str(vocab_size) + \".p\", \"wb\"))\n",
    "    pkl.dump(test_idx, open(\"test_idx_\" + token_mode + ' ' + str(n_gram) + optimizer + ' ' + str(vocab_size) + \".p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_idx_set(train_idx, val_idx, test_idx, TOKEN_MODE, NGRAM, MAX_VOCAB_SIZE, OPTIMIZER, current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_idx_set(token_mode, n_gram, current_path):\n",
    "    os.chdir(current_path)\n",
    "    train_idx = pkl.load(open(current_path + \"/train_idx_\" + token_mode + ' ' + str(n_gram) + \".p\", \"rb\"))\n",
    "    val_idx = pkl.load(open(current_path + \"/val_idx_\" + token_mode + ' ' + str(n_gram) + \".p\", \"rb\"))\n",
    "    test_idx = pkl.load(open(current_path + \"/test_idx_\" + token_mode + ' ' + str(n_gram) + \".p\", \"rb\"))\n",
    "    return train_idx, val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of train label is 20000 the value between switch points are 1 0\n",
      "The length of val label is 5000 the value between switch points are 1 0\n",
      "The length of test label is 25000 the value between switch points are 1 0\n"
     ]
    }
   ],
   "source": [
    "# Create labels for data sets\n",
    "def build_label(data_idx):\n",
    "    label_neg = []\n",
    "    label_pos = []\n",
    "    for i in range(int(len(data_idx)/2)):\n",
    "        label_pos.append(1)\n",
    "        label_neg.append(0)\n",
    "    return label_pos + label_neg\n",
    "\n",
    "# Check label list for three sets \n",
    "train_label = build_label(train_idx)\n",
    "val_label = build_label(val_idx)\n",
    "test_label = build_label(test_idx)\n",
    "print('The length of train label is {} the value between switch points are {} {}'.format(len(train_label), train_label[int(len(train_label)/2)-1], train_label[int(len(train_label)/2)]))\n",
    "print('The length of val label is {} the value between switch points are {} {}'.format(len(val_label), val_label[int(len(val_label)/2)-1], val_label[int(len(val_label)/2)]))\n",
    "print('The length of test label is {} the value between switch points are {} {}'.format(len(test_label), test_label[int(len(test_label)/2)-1], test_label[int(len(test_label)/2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Establishment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up maximum sentence length for the use of standardizing matirx format later in the program\n",
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "# Set up local class of dataset to use in this program \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class localDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_idx, label_list):\n",
    "        self.data_idx = data_idx\n",
    "        self.label_list = label_list\n",
    "        assert(len(self.data_idx) == len(self.label_list))\n",
    "    def __len__(self):\n",
    "        return len(self.data_idx)\n",
    "    def __getitem__(self, key):\n",
    "        review_idx = self.data_idx[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.label_list[key]\n",
    "        return [review_idx, len(review_idx), label]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    idx_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    for review in batch:\n",
    "        label_list.append(review[2])\n",
    "        length_list.append(review[1])\n",
    "        padded_review = np.pad(np.array(review[0]),pad_width=((0,MAX_SENTENCE_LENGTH-review[1])),mode=\"constant\", constant_values=0)\n",
    "        idx_list.append(padded_review)\n",
    "    return [torch.from_numpy(np.array(idx_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local dataset\n",
    "train_dataset = localDataset(train_idx, train_label)\n",
    "val_dataset = localDataset(val_idx, val_label)\n",
    "test_dataset = localDataset(test_idx, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "BATCH_SIZE = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=collate_fn,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIER_MODE = 'binary'\n",
    "# Establish the BagOfWords Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    def __init__(self, review_length, embed_dim):\n",
    "        super(BagOfWords, self).__init__()\n",
    "        self.embedding = nn.Embedding(review_length, embed_dim, padding_idx = 0)\n",
    "        self.linearReg = nn.Linear(embed_dim, 2)\n",
    "    def forward(self, idx_list, length_list):\n",
    "        embed_result_words = self.embedding(idx_list)\n",
    "        embed_result_review = torch.sum(embed_result_words, dim = 1)\n",
    "        embed_result = embed_result_review / length_list.view(length_list.size()[0],1).expand_as(embed_result_review).float()\n",
    "        \n",
    "        result = self.linearReg(embed_result.float())\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without Learning Annealing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BagOfWords(len(id2token), EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/4], Step: [101/313], Validation Acc: 50.34\n",
      "Epoch: [1/4], Step: [201/313], Validation Acc: 68.12\n",
      "Epoch: [1/4], Step: [301/313], Validation Acc: 58.42\n",
      "Epoch: [2/4], Step: [101/313], Validation Acc: 71.76\n",
      "Epoch: [2/4], Step: [201/313], Validation Acc: 72.64\n",
      "Epoch: [2/4], Step: [301/313], Validation Acc: 73.46\n",
      "Epoch: [3/4], Step: [101/313], Validation Acc: 72.56\n",
      "Epoch: [3/4], Step: [201/313], Validation Acc: 72.44\n",
      "Epoch: [3/4], Step: [301/313], Validation Acc: 72.42\n",
      "Epoch: [4/4], Step: [101/313], Validation Acc: 72.18\n",
      "Epoch: [4/4], Step: [201/313], Validation Acc: 71.04\n",
      "Epoch: [4/4], Step: [301/313], Validation Acc: 69.48\n",
      "Final Validation Acc: 71.0\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "if OPTIMIZER == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, NUM_EPOCHS, i+1, len(train_loader), val_acc))\n",
    "\n",
    "final_accuracy = test_model(val_loader, model)\n",
    "print('Final Validation Acc: {}'.format(final_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WIth Learning Rate Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BagOfWords(len(id2token), EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/4], Step: [101/313], Validation Acc: 83.6\n",
      "Epoch: [1/4], Step: [201/313], Validation Acc: 84.76\n",
      "Epoch: [1/4], Step: [301/313], Validation Acc: 83.54\n",
      "Epoch: [2/4], Step: [101/313], Validation Acc: 84.24\n",
      "Epoch: [2/4], Step: [201/313], Validation Acc: 84.2\n",
      "Epoch: [2/4], Step: [301/313], Validation Acc: 84.4\n",
      "Epoch: [3/4], Step: [101/313], Validation Acc: 84.48\n",
      "Epoch: [3/4], Step: [201/313], Validation Acc: 84.44\n",
      "Epoch: [3/4], Step: [301/313], Validation Acc: 84.44\n",
      "Epoch: [4/4], Step: [101/313], Validation Acc: 84.42\n",
      "Epoch: [4/4], Step: [201/313], Validation Acc: 84.42\n",
      "Epoch: [4/4], Step: [301/313], Validation Acc: 84.42\n",
      "Final Validation Acc: 84.42\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "lambda1 = lambda epoch: 0.1**epoch\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "if OPTIMIZER == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    \n",
    "scheduler = LambdaLR(optimizer, lr_lambda=[lambda1])\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        #scheduler.step()\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, NUM_EPOCHS, i+1, len(train_loader), val_acc))\n",
    "\n",
    "final_accuracy = test_model(val_loader, model)\n",
    "print('Final Validation Acc: {}'.format(final_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output function to write the result to file\n",
    "def result_output(test_loader, model, token_mode, n_gram, classifier_mode, current_path):\n",
    "    os.chdir(current_path)\n",
    "    fout = open('results.txt', 'a')\n",
    "    accuracy = test_model(test_loader, model)\n",
    "    fout.write('The accuracy for {} classifier {} {}gram tokenization is: {}'.format(classifier_mode, token_mode, str(n_gram), accuracy))\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_three_correct(loader, model):\n",
    "    model.eval()\n",
    "    correct_sample = []\n",
    "    correct_label = []\n",
    "    counter_c = 0\n",
    "    \n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        correction = list(predicted.eq(labels.view_as(predicted)))\n",
    "        label = list(labels)\n",
    "        data_content = list(data_batch)\n",
    "        inner_counter = 0\n",
    "        while inner_counter < len(correction):\n",
    "            if correction[inner_counter] == 1:\n",
    "                correct_sample.append(list(data_content[inner_counter].numpy()))\n",
    "                correct_label.append(label[inner_counter].item())\n",
    "                (data_content[inner_counter])\n",
    "                inner_counter += 1\n",
    "                counter_c += 1\n",
    "                if counter_c == 3:\n",
    "                    break\n",
    "            else:\n",
    "                inner_counter += 1\n",
    "        break\n",
    "    \n",
    "    return correct_sample, correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 934,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_sample, correct_label = list_three_correct(val_loader, model)\n",
    "len(correct_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcorrect = open('three_correct_reviews.txt','w')\n",
    "for i in range(3):\n",
    "    fcorrect.write('The review: ' + str(correct_sample[i]) + ' is correctly labeled as ' + str(correct_label[i]) + '\\n')\n",
    "fcorrect.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_three_incorrect(loader, model):\n",
    "    model.eval()\n",
    "    correct_sample = []\n",
    "    correct_label = []\n",
    "    counter_c = 0\n",
    "    \n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        correction = list(predicted.eq(labels.view_as(predicted)))\n",
    "        label = list(labels)\n",
    "        data_content = list(data_batch)\n",
    "        inner_counter = 0\n",
    "        while inner_counter < len(correction):\n",
    "            if correction[inner_counter] == 0:\n",
    "                correct_sample.append(list(data_content[inner_counter].numpy()))\n",
    "                correct_label.append(label[inner_counter].item())\n",
    "                (data_content[inner_counter])\n",
    "                inner_counter += 1\n",
    "                counter_c += 1\n",
    "                if counter_c == 3:\n",
    "                    break\n",
    "            else:\n",
    "                inner_counter += 1\n",
    "        break\n",
    "    \n",
    "    return correct_sample, correct_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_sample, incorrect_label = list_three_incorrect(val_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [],
   "source": [
    "fincorrect = open('three_incorrect_reviews.txt','w')\n",
    "for i in range(3):\n",
    "    fincorrect.write('The review: ' + str(incorrect_sample[i]) + ' is incorrectly labeled as ' + str(incorrect_label[i]) + '\\n')\n",
    "fincorrect.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BagOfWords(len(id2token), EMBED_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/4], Step: [101/313], Validation Acc: 81.72\n",
      "Epoch: [1/4], Step: [201/313], Validation Acc: 82.42\n",
      "Epoch: [1/4], Step: [301/313], Validation Acc: 84.18\n",
      "Epoch: [2/4], Step: [101/313], Validation Acc: 84.06\n",
      "Epoch: [2/4], Step: [201/313], Validation Acc: 84.08\n",
      "Epoch: [2/4], Step: [301/313], Validation Acc: 84.26\n",
      "Epoch: [3/4], Step: [101/313], Validation Acc: 84.16\n",
      "Epoch: [3/4], Step: [201/313], Validation Acc: 84.32\n",
      "Epoch: [3/4], Step: [301/313], Validation Acc: 84.26\n",
      "Epoch: [4/4], Step: [101/313], Validation Acc: 84.2\n",
      "Epoch: [4/4], Step: [201/313], Validation Acc: 84.2\n",
      "Epoch: [4/4], Step: [301/313], Validation Acc: 84.24\n",
      "Final Test Acc: 85.368\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "lambda1 = lambda epoch: 0.1**epoch\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "if OPTIMIZER == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    \n",
    "scheduler = LambdaLR(optimizer, lr_lambda=[lambda1])\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        #scheduler.step()\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, NUM_EPOCHS, i+1, len(train_loader), val_acc))\n",
    "\n",
    "final_accuracy = test_model(test_loader, model)\n",
    "print('Final Test Acc: {}'.format(final_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Acc: 86.536\n"
     ]
    }
   ],
   "source": [
    "final_accuracy = test_model(test_loader, model)\n",
    "print('Final Test Acc: {}'.format(final_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/4], Step: [51/313], Validation Acc: 77.6\n",
      "Epoch: [1/4], Step: [101/313], Validation Acc: 81.96\n",
      "Epoch: [1/4], Step: [151/313], Validation Acc: 83.02\n",
      "Epoch: [1/4], Step: [201/313], Validation Acc: 83.1\n",
      "Epoch: [1/4], Step: [251/313], Validation Acc: 84.22\n",
      "Epoch: [1/4], Step: [301/313], Validation Acc: 83.44\n",
      "Epoch: [2/4], Step: [51/313], Validation Acc: 83.82\n",
      "Epoch: [2/4], Step: [101/313], Validation Acc: 83.84\n",
      "Epoch: [2/4], Step: [151/313], Validation Acc: 84.12\n",
      "Epoch: [2/4], Step: [201/313], Validation Acc: 84.04\n",
      "Epoch: [2/4], Step: [251/313], Validation Acc: 83.94\n",
      "Epoch: [2/4], Step: [301/313], Validation Acc: 84.02\n",
      "Epoch: [3/4], Step: [51/313], Validation Acc: 84.16\n",
      "Epoch: [3/4], Step: [101/313], Validation Acc: 84.16\n",
      "Epoch: [3/4], Step: [151/313], Validation Acc: 84.1\n",
      "Epoch: [3/4], Step: [201/313], Validation Acc: 84.08\n",
      "Epoch: [3/4], Step: [251/313], Validation Acc: 84.04\n",
      "Epoch: [3/4], Step: [301/313], Validation Acc: 84.06\n",
      "Epoch: [4/4], Step: [51/313], Validation Acc: 84.08\n",
      "Epoch: [4/4], Step: [101/313], Validation Acc: 84.08\n",
      "Epoch: [4/4], Step: [151/313], Validation Acc: 84.08\n",
      "Epoch: [4/4], Step: [201/313], Validation Acc: 84.08\n",
      "Epoch: [4/4], Step: [251/313], Validation Acc: 84.08\n",
      "Epoch: [4/4], Step: [301/313], Validation Acc: 84.08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting learning curve\n",
    "model = BagOfWords(len(id2token), EMBED_DIM)\n",
    "\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "lambda1 = lambda epoch: 0.1**epoch\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "if OPTIMIZER == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    \n",
    "scheduler = LambdaLR(optimizer, lr_lambda=[lambda1])\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "training_example = []\n",
    "learning_accu = []\n",
    "record = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    scheduler.step()\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        #scheduler.step()\n",
    "        model.train()\n",
    "        record += len(list(data))\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 50 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, NUM_EPOCHS, i+1, len(train_loader), val_acc))\n",
    "            learning_accu.append(val_acc)\n",
    "            training_example.append(record)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = np.asarray(training_example)\n",
    "y = np.asarray(learning_accu)\n",
    "plt.plot(x, y)\n",
    "\n",
    "#plt.xlim(5, 0)  # decreasing time\n",
    "\n",
    "plt.xlabel('learning examples')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Learning Curve')\n",
    "#plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
